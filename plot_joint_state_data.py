"""Plot ROS CSV data generated by rosbag_to_csv.py.

USAGE MODES:
============

1. Single-trial mode (plot one CSV file):
        python plot_ros_data.py path/to/topic.csv [options]

2. Multi-trial overlay mode (overlay trials from one batch directory):
        python plot_ros_data.py ./data_root --batch "0-deg" --columns "position[0]" [options]

3. Multi-batch comparison mode (compare averages across multiple configurations):
        python plot_ros_data.py ./data_root --compare "0-deg" "30-deg" --columns "position[0]" [options]

PARAMETERS:
===========

Positional:
        csv_file                Path to a single CSV file (single-trial mode) OR
                                Path to a root directory containing batch folders (multi-trial mode).

Mode Selection:
        --batch DIR             Directory containing multiple trial folders.
        --compare DIR [DIR ...] Compare multiple batch directories on one plot.

Column Selection:
        --columns NAME [NAME ...] List of columns to plot (e.g., 'position[0]' 'effort[1]').
                                  If not provided, plots all columns (single-trial only).

Joint Selection (Alternative to --columns):
        --joint NAME            Select a specific joint by name (e.g., 'joint4').
                                Requires --channel to specify the attribute (position, velocity, effort).
                                Dynamically resolves the column index from the CSV data.
        --channel TYPE          Attribute to plot for the selected joint (position, velocity, effort).
                                Default: position.

Alignment & Clipping:
        --align                 Align trials by onset of movement (derivative threshold).
                                Requires exactly one column selected via --columns.
        --clip START END        Clip to absolute time range [START, END] seconds.
        --peak BEFORE AFTER     Clip around the peak value. Requires exactly one column.

Data Transformation:
        --flip                  Flip Y-axis values.
        --normalize             Normalize each trial to start at 0 (subtract initial value).

Plotting Options:
        --envelope              Show shaded min/max envelope around the mean.
        --max-trials N          Max trials to load per batch directory. Default: 5.
"""

import argparse
import csv
import re
from pathlib import Path

import numpy as np
import matplotlib.pyplot as plt

STATE_CHANNELS = ["position", "velocity", "effort"]

SMOOTHING_WINDOW = 6

# Universal Color Palette (Flat UI Extended)
COLOR_PALETTE = [
    "#F73535",  # Red
    "#3498DB",  # Blue
    "#2ECC71",  # Green
    "#F79235",  # Orange
    "#9B59B6",  # Purple
    "#1ABC9C",  # Cyan
    "#E91E63",  # Magenta
    "#FDD01D",  # Yellow
    "#16A085",  # Dark Teal
    "#8E44AD",  # Dark Purple
    "#D35400",  # Pumpkin
    "#7F8C8D",  # Gray
]

# Alignment onset detection thresholds
ONSET_NOISE_MULTIPLIER = 17.0
ONSET_MIN_FRACTION = 0.3

# Plot Customization Overrides
PLOT_TITLE = None
X_AXIS_LABEL = None
Y_AXIS_LABEL = None
LEGEND_LABELS = None


def resolve_batch_path(name: str, data_dir: Path | None = None) -> Path:
    """Resolve batch name to a directory path."""
    locations = []
    if data_dir:
        locations.append(data_dir)
    locations.append(Path("."))
    locations.append(Path(__file__).parent)

    for loc in locations:
        target = loc / name
        if target.is_dir():
            return target

    raise SystemExit(f"Directory not found for '{name}'. Checked in {[str(l) for l in locations]}")


def load_ros_csv(
    path: Path, 
    columns: list[str] | None = None, 
    joint_name: str | None = None,
    channel_type: str = "position"
):
    """Load ROS CSV and return time array and data columns.

    Args:
        path: Path to CSV file
        columns: List of specific column names to load
        joint_name: Name of joint to find (e.g. "joint4")
        channel_type: Type of data to load for joint (position, velocity, effort)

    Returns:
        t: list[float]          -- time in seconds
        data: dict[str, list]   -- keys are column names
    """
    data = {}
    t = []
    
    with path.open("r", newline="") as f:
        reader = csv.DictReader(f)
        if reader.fieldnames is None:
            raise RuntimeError(f"CSV file has no header: {path}")
        
        all_fields = reader.fieldnames
        if "time" not in all_fields:
             raise RuntimeError(f"CSV file missing 'time' column: {path}")

        # We need to read the first row to resolve joint names if requested
        try:
            first_row = next(reader)
        except StopIteration:
            raise RuntimeError("CSV file is empty (no data rows).")

        selected_fields = []
        
        # 1. Resolve joint name if provided
        if joint_name:
            # Find which index corresponds to this joint name
            # Look for columns like name[0], name[1], etc.
            found_index = None
            for field in all_fields:
                if field.startswith("name[") and field.endswith("]"):
                    if first_row.get(field) == joint_name:
                        # Extract index from "name[X]"
                        try:
                            idx_str = field[5:-1] # remove name[ and ]
                            found_index = int(idx_str)
                            break
                        except ValueError:
                            continue
            
            if found_index is not None:
                if channel_type.lower() == "all":
                    # Select all columns that end with [found_index] (e.g. position[4], velocity[4])
                    suffix = f"[{found_index}]"
                    for f in all_fields:
                        if f.endswith(suffix) and not f.startswith("name"):
                            selected_fields.append(f)
                    if not selected_fields:
                         raise RuntimeError(f"No data columns found for joint '{joint_name}' (index {found_index}).")
                else:
                    target_col = f"{channel_type}[{found_index}]"
                    if target_col in all_fields:
                        selected_fields.append(target_col)
                    else:
                        raise RuntimeError(f"Joint '{joint_name}' found at index {found_index}, but column '{target_col}' does not exist.")
            else:
                raise RuntimeError(f"Joint '{joint_name}' not found in any 'name[...]' column in the first row.")

        # 2. Add explicit columns
        if columns:
            for col in columns:
                if col not in all_fields:
                    print(f"Warning: Column '{col}' not found in CSV.")
                else:
                    if col not in selected_fields:
                        selected_fields.append(col)
        
        # 3. Use channel_type as a filter if no other selection method is used
        # If user didn't specify joint or columns, we use channel_type (default "position")
        # to select all columns starting with that type (case-insensitive).
        if not selected_fields and not joint_name and not columns:
            # Default behavior: select all columns matching the channel_type prefix
            # e.g. "position" -> "position[0]", "position[1]"...
            prefix = channel_type.lower()
            for f in all_fields:
                if f.lower().startswith(prefix) and f != "time" and not f.startswith("name["):
                    selected_fields.append(f)
            
            # If still nothing (maybe channel_type was "all" or something invalid), fallback to everything?
            # Or just let it fail/warn?
            # If channel_type is "all", we load everything.
            if channel_type.lower() == "all" or not selected_fields:
                 if channel_type.lower() == "all":
                     selected_fields = [f for f in all_fields if f != "time" and not f.startswith("name[")]

        if not selected_fields:
            raise RuntimeError(f"No columns selected to plot. (Channel filter: '{channel_type}')")

        # Initialize lists
        for field in selected_fields:
            data[field] = []

        # Process first row
        try:
            t_val = float(first_row["time"])
            t.append(t_val)
            for field in selected_fields:
                data[field].append(float(first_row[field]))
        except (ValueError, KeyError):
            pass # Skip malformed first row

        # Process remaining rows
        for row in reader:
            try:
                t_val = float(row["time"])
                t.append(t_val)
                for field in selected_fields:
                    data[field].append(float(row[field]))
            except (ValueError, KeyError):
                continue

    if not t:
        raise RuntimeError("No valid data rows found in CSV.")

    return t, data


def normalize_to_zero(data: dict[str, list[float]]):
    """Normalize each channel so that its first value becomes zero."""
    norm = {}
    for name, values in data.items():
        if not values:
            norm[name] = []
            continue
        offset = values[0]
        norm[name] = [v - offset for v in values]
    return norm


def flip_data(data: dict[str, list[float]]):
    """Flip (negate) all values in each channel."""
    return {name: [-v for v in values] for name, values in data.items()}


def smooth_data(data: dict[str, list[float]], window: int):
    """Apply moving average smoothing."""
    if window <= 1:
        return data

    smoothed = {}
    for name, values in data.items():
        if len(values) < window:
            smoothed[name] = values
            continue
        kernel = np.ones(window) / window
        smoothed[name] = np.convolve(values, kernel, mode='valid').tolist()
    return smoothed


def clip_data(t: list[float], data: dict[str, list[float]], t_start: float, t_end: float):
    """Clip time and data arrays to specified time range."""
    t_arr = np.array(t)
    mask = (t_arr >= t_start) & (t_arr <= t_end)
    indices = np.where(mask)[0]

    if len(indices) == 0:
        raise ValueError(f"No data found in time range [{t_start}, {t_end}]")

    t_clipped = [t[i] for i in indices]
    # Reset time so the clipped region starts at 0
    t0 = t_clipped[0]
    t_clipped = [tv - t0 for tv in t_clipped]
    
    data_clipped = {name: [values[i] for i in indices] for name, values in data.items()}

    return t_clipped, data_clipped


def clip_data_preserve_time(
    t: list[float],
    data: dict[str, list[float]],
    t_start: float,
    t_end: float,
):
    """Clip data to a time range WITHOUT resetting time to 0.

    Args:
        t: Time array
        data: Dictionary of signal arrays (same length as t)
        t_start: Start time (inclusive)
        t_end: End time (inclusive)

    Returns:
        Tuple of (clipped_time, clipped_data) - time values are preserved
    """
    t_arr = np.array(t)
    mask = (t_arr >= t_start) & (t_arr <= t_end)
    indices = np.where(mask)[0]

    if len(indices) == 0:
        raise ValueError(f"No data found in time range [{t_start}, {t_end}]")

    t_clipped = [t[i] for i in indices]
    data_clipped = {name: [values[i] for i in indices] for name, values in data.items()}

    return t_clipped, data_clipped


def clip_around_peak(
    t: list[float],
    data: dict[str, list[float]],
    channel: str,
    before: float,
    after: float,
):
    """Clip data around the peak of a channel."""
    if channel not in data:
        raise ValueError(f"Channel {channel} not found in data.")

    y = np.array(data[channel])
    idx_peak = int(np.argmax(np.abs(y)))
    t_peak = t[idx_peak]

    t_start = t_peak - before
    t_end = t_peak + after

    t_arr = np.array(t)
    mask = (t_arr >= t_start) & (t_arr <= t_end)
    indices = np.where(mask)[0]

    if len(indices) == 0:
        raise ValueError(f"No data found in peak range [{t_start}, {t_end}]")

    t_clipped = [t[i] for i in indices]
    t_clipped = [tv - t_peak for tv in t_clipped]

    data_clipped = {name: [values[i] for i in indices] for name, values in data.items()}

    return t_clipped, data_clipped


def compute_derivative(t: list[float], y: list[float]) -> tuple[list[float], list[float]]:
    """Compute numerical derivative dy/dt."""
    if len(t) < 3 or len(y) < 3:
        return [], []

    t_arr = np.array(t, dtype=float)
    y_arr = np.array(y, dtype=float)
    dt = np.diff(t_arr)
    dt[dt == 0.0] = np.min(dt[dt > 0.0]) if np.any(dt > 0.0) else 1.0

    dy = y_arr[2:] - y_arr[:-2]
    dt_mid = t_arr[2:] - t_arr[:-2]
    dt_mid[dt_mid == 0.0] = np.min(dt_mid[dt_mid > 0.0]) if np.any(dt_mid > 0.0) else 1.0

    dy_dt = dy / dt_mid
    t_mid = t_arr[1:-1]
    return t_mid.tolist(), dy_dt.tolist()


def align_trials_by_derivative_peak(
    trials: list[tuple[list[float], dict[str, list[float]]]],
    channel: str,
):
    """Align trials by onset of movement in a given channel."""
    if not trials:
        raise ValueError("No trials provided for alignment.")

    shifted_trials = []
    
    for trial_t, trial_data in trials:
        if channel not in trial_data:
            raise ValueError(f"Channel {channel} not in trial data.")
        
        t_mid, dy_dt = compute_derivative(trial_t, trial_data[channel])
        if not t_mid:
            raise ValueError("Not enough samples to compute derivative.")
        
        dy_dt_arr = np.abs(dy_dt)
        baseline_samples = max(10, len(dy_dt_arr) // 10)
        noise_floor = np.std(dy_dt_arr[:baseline_samples])
        
        threshold = max(
            ONSET_NOISE_MULTIPLIER * noise_floor,
            ONSET_MIN_FRACTION * np.max(dy_dt_arr)
        )
        
        onset_indices = np.where(dy_dt_arr > threshold)[0]
        if len(onset_indices) == 0:
            idx_onset = int(np.argmax(dy_dt_arr))
        else:
            idx_onset = onset_indices[0]
        
        t_onset = t_mid[idx_onset]
        shifted_t = [ti - t_onset for ti in trial_t]
        shifted_trials.append((shifted_t, trial_data))

    return shifted_trials


def load_and_preprocess_trial(path: Path, args, apply_clip=True):
    """Load and preprocess a single trial."""
    t, data = load_ros_csv(
        path, 
        columns=args.columns, 
        joint_name=args.joint,
        channel_type=args.channel
    )
    
    if args.normalize:
        data = normalize_to_zero(data)

    if args.flip:
        data = flip_data(data)

    if SMOOTHING_WINDOW > 1:
        data = smooth_data(data, SMOOTHING_WINDOW)
        t = t[SMOOTHING_WINDOW - 1 :]

    if apply_clip and args.clip:
        t_start, t_end = args.clip
        t, data = clip_data(t, data, t_start, t_end)

    return t, data


def process_batch_dir(batch_dir: Path, args):
    """Process a batch directory containing trial subdirectories."""
    if not batch_dir.is_dir():
        raise SystemExit(f"Batch directory not found: {batch_dir}")

    # Look for CSVs recursively or in trial subfolders
    # Structure: batch_dir/trial_X/*.csv
    csv_paths = sorted(batch_dir.rglob("*.csv"))
    
    if not csv_paths:
        raise SystemExit(f"No .csv files found in {batch_dir}")

    csv_paths = csv_paths[: args.max_trials]
    trials = []

    for path in csv_paths:
        t, data = load_and_preprocess_trial(path, args, apply_clip=False)
        trials.append((t, data))

    # Alignment
    if args.align:
        # Need exactly one channel to align on
        align_channel = None
        if args.columns and len(args.columns) == 1:
            align_channel = args.columns[0]
        elif trials and len(trials[0][1]) == 1:
            align_channel = list(trials[0][1].keys())[0]
        
        if not align_channel:
             raise SystemExit("--align requires exactly one column to be selected (via --columns).")

        try:
            trials = align_trials_by_derivative_peak(trials, channel=align_channel)
        except ValueError as e:
            print(f"Warning: Alignment failed: {e}")

    # Clip after alignment
    if args.clip:
        t_start, t_end = args.clip
        clipped_trials = []
        for t, data in trials:
            try:
                t_c, d_c = clip_data_preserve_time(t, data, t_start, t_end)
                clipped_trials.append((t_c, d_c))
            except ValueError:
                pass  # Skip trials that don't have data in this range
        trials = clipped_trials

    # Peak clipping
    if args.peak:
        peak_channel = None
        if args.columns and len(args.columns) == 1:
            peak_channel = args.columns[0]
        elif trials and len(trials[0][1]) == 1:
            peak_channel = list(trials[0][1].keys())[0]
            
        if not peak_channel:
             raise SystemExit("--peak requires exactly one column to be selected.")
             
        before, after = args.peak
        clipped_trials = []
        for t, data in trials:
            t_c, d_c = clip_around_peak(t, data, peak_channel, before, after)
            clipped_trials.append((t_c, d_c))
        trials = clipped_trials

    return trials


def compute_batch_stats(trials, channel):
    """Compute stats for a single channel across trials."""
    if not trials:
        raise ValueError("No trials.")

    # Find common time range (union - full range)
    t_min = min(min(t) for t, _ in trials)
    t_max = max(max(t) for t, _ in trials)

    t0, _ = trials[0]
    avg_dt = (t0[-1] - t0[0]) / (len(t0) - 1) if len(t0) > 1 else 0.001
    num_points = int((t_max - t_min) / avg_dt) + 1
    t_common = np.linspace(t_min, t_max, num_points)

    interpolated = []
    for t, data in trials:
        if channel not in data:
            raise ValueError(f"Channel {channel} missing.")
        y_interp = np.interp(t_common, t, data[channel])
        interpolated.append(y_interp)

    stacked = np.array(interpolated)
    return t_common, np.mean(stacked, axis=0), np.min(stacked, axis=0), np.max(stacked, axis=0)


def plot_multi_batch_envelope(batch_data, channel, show_envelope=True, title=None):
    fig, ax = plt.subplots(figsize=(12, 6))

    for idx, (label, t, mean, ymin, ymax) in enumerate(batch_data):
        color = COLOR_PALETTE[idx % len(COLOR_PALETTE)]
        
        if LEGEND_LABELS and idx < len(LEGEND_LABELS):
            label = LEGEND_LABELS[idx]

        if show_envelope:
            ax.fill_between(t, ymin, ymax, alpha=0.25, color=color)
        ax.plot(t, mean, linewidth=2, label=label, color=color)

    ax.set_xlabel(X_AXIS_LABEL if X_AXIS_LABEL else "Time [s]")
    ax.set_ylabel(Y_AXIS_LABEL if Y_AXIS_LABEL else channel)
    ax.grid(True, linestyle="--", alpha=0.3)
    ax.legend()
    
    final_title = PLOT_TITLE if PLOT_TITLE else title
    if final_title:
        fig.suptitle(final_title)
    plt.tight_layout()
    plt.show()


def plot_trials(trials, title=None):
    """Plot all channels for all trials, grouped by channel type (case-insensitive)."""
    if not trials:
        return

    # 1. Identify Groups (e.g. "position", "velocity")
    all_columns = list(trials[0][1].keys())
    groups = {}
    
    for col in all_columns:
        # Extract prefix before '[' or use full name
        match = re.match(r"([^\[]+)(?:\[.*\])?", col)
        prefix = match.group(1) if match else col
        
        # Normalize to lowercase for grouping to handle "Position", "position", etc. together
        key = prefix.lower()
        
        if key not in groups:
            groups[key] = []
        groups[key].append(col)
    
    # 2. Create Subplots (one per group)
    fig, axes = plt.subplots(len(groups), 1, sharex=True, figsize=(10, 4 * len(groups)))
    if len(groups) == 1:
        axes = [axes]
    
    # 3. Plot
    for ax, (group_key, cols) in zip(axes, groups.items()):
        # Use Title Case for the Y-label (e.g. "Position")
        group_label = group_key.title()
        
        # Sort columns naturally if possible (e.g. pos[0], pos[1]...)
        # Simple sort works for single digit indices, but pos[10] < pos[2] string-wise.
        # Let's just trust the order from CSV or args for now.
        
        plot_idx = 0
        for i, col in enumerate(cols):
            for j, (t, data) in enumerate(trials):
                # Color Logic:
                # Single Trial: Color by Column Index (reuse palette for each subplot)
                # Multi Trial: Color by Trial Index
                
                if len(trials) == 1:
                    color = COLOR_PALETTE[i % len(COLOR_PALETTE)]
                    label = col
                else:
                    color = COLOR_PALETTE[j % len(COLOR_PALETTE)]
                    # If multiple columns in multi-trial, legend is tricky.
                    # If only 1 column per group (common case for multi-trial), label is trial.
                    if len(cols) == 1:
                        label = f"trial {j+1}"
                    else:
                        label = f"{col} (T{j+1})"
                
                if LEGEND_LABELS and plot_idx < len(LEGEND_LABELS):
                    label = LEGEND_LABELS[plot_idx]

                ax.plot(t, data[col], label=label, alpha=1.0, color=color)
                plot_idx += 1
        
        ax.set_ylabel(Y_AXIS_LABEL if Y_AXIS_LABEL else group_label)
        ax.grid(True, linestyle="--", alpha=0.3)
        ax.legend(loc="best", fontsize="small", ncol=2)
    
    axes[-1].set_xlabel(X_AXIS_LABEL if X_AXIS_LABEL else "Time [s]")
    
    final_title = PLOT_TITLE if PLOT_TITLE else title
    if final_title:
        fig.suptitle(final_title)
    plt.tight_layout()
    plt.show()


def main():
    parser = argparse.ArgumentParser(description="Plot ROS CSV data.")
    parser.add_argument("csv_file", nargs="?", help="[Single] CSV file path OR [Batch/Compare] Data Root Directory")
    parser.add_argument("--batch", help="[Batch] Batch directory name (inside data root)")
    parser.add_argument("--compare", nargs="+", help="[Compare] Compare multiple batch directories")
    parser.add_argument("--columns", nargs="+", help="[All] Specific columns to plot")
    parser.add_argument("--joint", help="[All] Joint name (e.g. 'joint4')")
    parser.add_argument(
        "--channel", 
        default="all", 
        help="[All] Channel type to plot (position, velocity, effort, all). "
             "If --joint is used, selects that attribute for the joint. "
             "Otherwise, selects ALL columns starting with this type. "
             "Default: all"
    )
    parser.add_argument("--clip", nargs=2, type=float, help="[All] Clip time range: START END")
    parser.add_argument("--peak", nargs=2, type=float, help="[Batch] Clip around peak: BEFORE AFTER")
    parser.add_argument("--align", action="store_true", help="[Batch] Align trials by derivative onset")
    parser.add_argument("--flip", action="store_true", help="[All] Flip Y-axis values")
    parser.add_argument("--normalize", action="store_true", help="[All] Normalize to start at 0")
    parser.add_argument("--envelope", action="store_true", help="[Batch/Compare] Show min/max envelope")
    parser.add_argument("--max-trials", type=int, default=5, help="[Batch/Compare] Max trials to load per batch")
    
    args = parser.parse_args()

    data_root = None
    if args.csv_file and Path(args.csv_file).is_dir():
        data_root = Path(args.csv_file)

    # Comparison Mode
    if args.compare:
        # For comparison, we usually want to compare ONE metric across batches
        # So we require exactly one column/channel
        target_channel = None
        if args.columns and len(args.columns) == 1:
            target_channel = args.columns[0]
        elif args.joint:
            # If joint is specified, we don't know the exact column name yet (it depends on the file)
            # But we can assume the user wants to compare the attribute they asked for.
            # However, the column name might differ between files if indices change (unlikely for same robot).
            # We'll let process_batch_dir handle loading, but we need a label for plotting.
            # We can use f"{args.channel} ({args.joint})" as a label if we can't get the exact column name easily.
            pass 
        
        # If we used --joint, the column name is resolved per-file.
        # But for plotting, we need to know what to extract from the 'data' dict.
        # The 'data' dict keys will be like "position[5]".
        # If we use --joint, load_ros_csv returns a dict with ONE key (if we only asked for one joint).
        
        batch_data = []
        for batch_name in args.compare:
            batch_dir = resolve_batch_path(batch_name, data_root)
            trials = process_batch_dir(batch_dir, args)
            
            # Determine which channel to aggregate
            if not trials:
                continue
                
            # If using --joint, the key in data dict might vary if indices vary (unlikely).
            # But usually it's the only key if we didn't ask for anything else.
            first_trial_data = trials[0][1]
            if len(first_trial_data) == 1:
                target_channel = list(first_trial_data.keys())[0]
            elif args.columns and len(args.columns) == 1:
                target_channel = args.columns[0]
            else:
                 raise SystemExit("Comparison mode requires exactly one signal to be selected (via --columns or --joint).")

            stats = compute_batch_stats(trials, target_channel)
            batch_data.append((batch_name, *stats))
            
        if not batch_data:
            print("No data found for comparison.")
            return

        # Only normalize time if using --align, otherwise plot raw data
        if args.align:
            global_min_t = min(item[1][0] for item in batch_data)
            new_batch_data = []
            for label, t, y_mean, y_min, y_max in batch_data:
                t_shifted = t - global_min_t
                new_batch_data.append((label, t_shifted, y_mean, y_min, y_max))
            batch_data = new_batch_data

        # Use a generic label if target_channel varies or is obscure
        plot_label = target_channel
        if args.joint:
            plot_label = f"{args.channel} of {args.joint}"

        plot_multi_batch_envelope(batch_data, plot_label, args.envelope, 
                                  title=f"Comparison: {plot_label}")
        return

    # Batch Mode
    if args.batch:
        batch_dir = resolve_batch_path(args.batch, data_root)
        trials = process_batch_dir(batch_dir, args)
        
        # Normalize time across trials so the earliest point is at 0
        if trials:
            global_min_t = min(t[0] for t, _ in trials)
            shifted_trials = []
            for t, data in trials:
                new_t = [tv - global_min_t for tv in t]
                shifted_trials.append((new_t, data))
            trials = shifted_trials

        if args.envelope:
            # Envelope implies averaging, so we need one channel usually
            # Or we can plot envelope for each channel?
            # Let's support single channel envelope for now
            if len(trials[0][1]) == 1:
                channel = list(trials[0][1].keys())[0]
                stats = compute_batch_stats(trials, channel)
                plot_multi_batch_envelope([(args.batch, *stats)], channel, True, 
                                          title=f"Batch: {args.batch}")
            else:
                print("Envelope mode requires exactly one channel. Plotting raw trials instead.")
                plot_trials(trials, title=f"Batch: {args.batch}")
        else:
            plot_trials(trials, title=f"Batch: {args.batch}")
        return

    # Single File Mode
    if args.csv_file and Path(args.csv_file).is_file():
        t, data = load_and_preprocess_trial(Path(args.csv_file), args, apply_clip=True)
        plot_trials([(t, data)], title=Path(args.csv_file).name)
        return

    parser.print_help()


if __name__ == "__main__":
    main()
